{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30657ab1-180a-4fd5-bfca-5202bf3409cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ec039-099a-4877-b485-cff97b3d49ae",
   "metadata": {},
   "source": [
    "### Loading and visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6605bd1d-4046-4880-9bb7-a51e3b40f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(root=\"../data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST(root=\"../data\", train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab976d2-fe44-4ead-a19a-15578068c3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "img, label = training_data[sample_idx]\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103c5511-62b2-422a-89d3-1c5a8aa2a182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e12a6a-625d-43d0-9b55-cef1fea94fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09559be-21e3-4a5d-bd71-04c89a169127",
   "metadata": {},
   "source": [
    "### Defining our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e4b130-2014-47a8-896c-bf61d0e84cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1) #nn.Conv2d(1, 32, 3, 1, padding=(1,1)) for the padded version\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        ## after the 1st convolution, the ouput will have a size of (32, 26, 26)\n",
    "        ## after the 2nd convolution, the ouput will have a size of (64, 24, 24)\n",
    "        ## after pooling, the output will have a size of (64, 12, 12)\n",
    "        ## after flatenning, the ouputput will have a size of 64 * 12 * 12 = 9216\n",
    "        self.fc1 = nn.Linear(9216, 128) \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59945f80-54fd-48f1-8d5b-c1c79cc3367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be6de2-4dd4-4c05-80c0-8b06347ab402",
   "metadata": {},
   "source": [
    "### How to debug/develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52e54de-7125-4d37-8aab-eae3cf9ede2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.Conv2d(1, 32, 3, 1, padding=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8bb4ab5-fb67-404f-8063-4b7454287351",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = l(training_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f39e8be9-4815-4d9b-a2e4-567a3c182b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0470c9-9e86-4379-9b9f-2f2841ca47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = nn.Conv2d(32, 64, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fddf688-b634-4c1f-a2f6-33f433c42c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 26, 26])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c205fe5-9fc9-447b-80c9-bbaa792a3705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 * 24 * 24 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f388c68-2949-4e71-8282-595fe71354eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece37265-7615-4304-8079-ce60dfda65d9",
   "metadata": {},
   "source": [
    "### Writing training and evaluation codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc97a3b-ef47-4950-9701-ed3d144b2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} Loss: {loss.item():.6f}')\n",
    "            \n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_data)}')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be406ba7-bdd0-40bd-959a-fd5e80357fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c44426-8077-4871-b923-d5c00dd12c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1199882"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc59e0d-fc88-4239-b721-1ef1bc90b151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_data[0]\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9a2927a-4aba-44cb-9f88-4bc27119f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_result = model(test_data[0][0].unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73248e83-4932-4dc5-8f7b-a53648085d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_result.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d429ca5a-5217-4489-a01a-aacf9a3a6ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86fd25b4-eb71-4b53-81b1-b592f0a0bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Loss: 2.313071\n",
      "Train Epoch: 1 Loss: 0.472216\n",
      "Train Epoch: 1 Loss: 0.175738\n",
      "Train Epoch: 1 Loss: 0.311754\n",
      "Train Epoch: 1 Loss: 0.123183\n",
      "Train Epoch: 1 Loss: 0.413600\n",
      "Train Epoch: 1 Loss: 0.119312\n",
      "Train Epoch: 1 Loss: 0.133769\n",
      "Train Epoch: 1 Loss: 0.191937\n",
      "Train Epoch: 1 Loss: 0.087185\n",
      "Train Epoch: 1 Loss: 0.250072\n",
      "Train Epoch: 1 Loss: 0.138338\n",
      "Train Epoch: 1 Loss: 0.231149\n",
      "Train Epoch: 1 Loss: 0.107408\n",
      "Train Epoch: 1 Loss: 0.062320\n",
      "Train Epoch: 1 Loss: 0.158484\n",
      "Train Epoch: 1 Loss: 0.018901\n",
      "Train Epoch: 1 Loss: 0.018791\n",
      "Train Epoch: 1 Loss: 0.036233\n",
      "\n",
      "Test set: Average loss: 0.0522, Accuracy: 9836/10000\n",
      "Train Epoch: 2 Loss: 0.042613\n",
      "Train Epoch: 2 Loss: 0.180454\n",
      "Train Epoch: 2 Loss: 0.170956\n",
      "Train Epoch: 2 Loss: 0.050528\n",
      "Train Epoch: 2 Loss: 0.039188\n",
      "Train Epoch: 2 Loss: 0.159708\n",
      "Train Epoch: 2 Loss: 0.049851\n",
      "Train Epoch: 2 Loss: 0.010633\n",
      "Train Epoch: 2 Loss: 0.129443\n",
      "Train Epoch: 2 Loss: 0.034566\n",
      "Train Epoch: 2 Loss: 0.151554\n",
      "Train Epoch: 2 Loss: 0.086441\n",
      "Train Epoch: 2 Loss: 0.237784\n",
      "Train Epoch: 2 Loss: 0.116316\n",
      "Train Epoch: 2 Loss: 0.034815\n",
      "Train Epoch: 2 Loss: 0.109037\n",
      "Train Epoch: 2 Loss: 0.033428\n",
      "Train Epoch: 2 Loss: 0.029189\n",
      "Train Epoch: 2 Loss: 0.022019\n",
      "\n",
      "Test set: Average loss: 0.0407, Accuracy: 9864/10000\n",
      "Train Epoch: 3 Loss: 0.060679\n",
      "Train Epoch: 3 Loss: 0.070839\n",
      "Train Epoch: 3 Loss: 0.145186\n",
      "Train Epoch: 3 Loss: 0.031953\n",
      "Train Epoch: 3 Loss: 0.145095\n",
      "Train Epoch: 3 Loss: 0.135925\n",
      "Train Epoch: 3 Loss: 0.021279\n",
      "Train Epoch: 3 Loss: 0.010030\n",
      "Train Epoch: 3 Loss: 0.004548\n",
      "Train Epoch: 3 Loss: 0.020397\n",
      "Train Epoch: 3 Loss: 0.175949\n",
      "Train Epoch: 3 Loss: 0.032408\n",
      "Train Epoch: 3 Loss: 0.200453\n",
      "Train Epoch: 3 Loss: 0.074475\n",
      "Train Epoch: 3 Loss: 0.026281\n",
      "Train Epoch: 3 Loss: 0.021232\n",
      "Train Epoch: 3 Loss: 0.015268\n",
      "Train Epoch: 3 Loss: 0.027386\n",
      "Train Epoch: 3 Loss: 0.081692\n",
      "\n",
      "Test set: Average loss: 0.0390, Accuracy: 9877/10000\n",
      "Train Epoch: 4 Loss: 0.060596\n",
      "Train Epoch: 4 Loss: 0.055776\n",
      "Train Epoch: 4 Loss: 0.237661\n",
      "Train Epoch: 4 Loss: 0.037002\n",
      "Train Epoch: 4 Loss: 0.023059\n",
      "Train Epoch: 4 Loss: 0.117745\n",
      "Train Epoch: 4 Loss: 0.015045\n",
      "Train Epoch: 4 Loss: 0.021460\n",
      "Train Epoch: 4 Loss: 0.063065\n",
      "Train Epoch: 4 Loss: 0.075820\n",
      "Train Epoch: 4 Loss: 0.172663\n",
      "Train Epoch: 4 Loss: 0.007877\n",
      "Train Epoch: 4 Loss: 0.157775\n",
      "Train Epoch: 4 Loss: 0.128456\n",
      "Train Epoch: 4 Loss: 0.038852\n",
      "Train Epoch: 4 Loss: 0.089082\n",
      "Train Epoch: 4 Loss: 0.064330\n",
      "Train Epoch: 4 Loss: 0.007147\n",
      "Train Epoch: 4 Loss: 0.059374\n",
      "\n",
      "Test set: Average loss: 0.0389, Accuracy: 9876/10000\n",
      "Train Epoch: 5 Loss: 0.024974\n",
      "Train Epoch: 5 Loss: 0.101890\n",
      "Train Epoch: 5 Loss: 0.077810\n",
      "Train Epoch: 5 Loss: 0.063691\n",
      "Train Epoch: 5 Loss: 0.060792\n",
      "Train Epoch: 5 Loss: 0.123913\n",
      "Train Epoch: 5 Loss: 0.053951\n",
      "Train Epoch: 5 Loss: 0.081101\n",
      "Train Epoch: 5 Loss: 0.021642\n",
      "Train Epoch: 5 Loss: 0.069163\n",
      "Train Epoch: 5 Loss: 0.133004\n",
      "Train Epoch: 5 Loss: 0.061455\n",
      "Train Epoch: 5 Loss: 0.166113\n",
      "Train Epoch: 5 Loss: 0.146592\n",
      "Train Epoch: 5 Loss: 0.011059\n",
      "Train Epoch: 5 Loss: 0.021712\n",
      "Train Epoch: 5 Loss: 0.050279\n",
      "Train Epoch: 5 Loss: 0.003384\n",
      "Train Epoch: 5 Loss: 0.049550\n",
      "\n",
      "Test set: Average loss: 0.0389, Accuracy: 9876/10000\n",
      "Train Epoch: 6 Loss: 0.032433\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     test(model, test_loader)\n\u001b[1;32m      6\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda/envs/nobleprog_training/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/nobleprog_training/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "for epoch in range(1, 10 + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f065ea-cb01-477d-918a-762ae1db5b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nobleprog_training",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
