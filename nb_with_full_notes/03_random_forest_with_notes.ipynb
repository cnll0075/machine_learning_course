{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous notebook, we split our data into the same train and validation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df = pd.read_csv(\"data/Titanic/train_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['Survived']\n",
    "x = train_df.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=23, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gini'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sqrt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mForestClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    A random forest classifier.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    A random forest is a meta estimator that fits a number of decision tree\u001b[0m\n",
       "\u001b[0;34m    classifiers on various sub-samples of the dataset and uses averaging to\u001b[0m\n",
       "\u001b[0;34m    improve the predictive accuracy and control over-fitting.\u001b[0m\n",
       "\u001b[0;34m    The sub-sample size is controlled with the `max_samples` parameter if\u001b[0m\n",
       "\u001b[0;34m    `bootstrap=True` (default), otherwise the whole dataset is used to build\u001b[0m\n",
       "\u001b[0;34m    each tree.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Read more in the :ref:`User Guide <forest>`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Parameters\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    n_estimators : int, default=100\u001b[0m\n",
       "\u001b[0;34m        The number of trees in the forest.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionchanged:: 0.22\u001b[0m\n",
       "\u001b[0;34m           The default value of ``n_estimators`` changed from 10 to 100\u001b[0m\n",
       "\u001b[0;34m           in 0.22.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\u001b[0m\n",
       "\u001b[0;34m        The function to measure the quality of a split. Supported criteria are\u001b[0m\n",
       "\u001b[0;34m        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\u001b[0m\n",
       "\u001b[0;34m        Shannon information gain, see :ref:`tree_mathematical_formulation`.\u001b[0m\n",
       "\u001b[0;34m        Note: This parameter is tree-specific.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    max_depth : int, default=None\u001b[0m\n",
       "\u001b[0;34m        The maximum depth of the tree. If None, then nodes are expanded until\u001b[0m\n",
       "\u001b[0;34m        all leaves are pure or until all leaves contain less than\u001b[0m\n",
       "\u001b[0;34m        min_samples_split samples.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    min_samples_split : int or float, default=2\u001b[0m\n",
       "\u001b[0;34m        The minimum number of samples required to split an internal node:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - If int, then consider `min_samples_split` as the minimum number.\u001b[0m\n",
       "\u001b[0;34m        - If float, then `min_samples_split` is a fraction and\u001b[0m\n",
       "\u001b[0;34m          `ceil(min_samples_split * n_samples)` are the minimum\u001b[0m\n",
       "\u001b[0;34m          number of samples for each split.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionchanged:: 0.18\u001b[0m\n",
       "\u001b[0;34m           Added float values for fractions.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    min_samples_leaf : int or float, default=1\u001b[0m\n",
       "\u001b[0;34m        The minimum number of samples required to be at a leaf node.\u001b[0m\n",
       "\u001b[0;34m        A split point at any depth will only be considered if it leaves at\u001b[0m\n",
       "\u001b[0;34m        least ``min_samples_leaf`` training samples in each of the left and\u001b[0m\n",
       "\u001b[0;34m        right branches.  This may have the effect of smoothing the model,\u001b[0m\n",
       "\u001b[0;34m        especially in regression.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - If int, then consider `min_samples_leaf` as the minimum number.\u001b[0m\n",
       "\u001b[0;34m        - If float, then `min_samples_leaf` is a fraction and\u001b[0m\n",
       "\u001b[0;34m          `ceil(min_samples_leaf * n_samples)` are the minimum\u001b[0m\n",
       "\u001b[0;34m          number of samples for each node.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionchanged:: 0.18\u001b[0m\n",
       "\u001b[0;34m           Added float values for fractions.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    min_weight_fraction_leaf : float, default=0.0\u001b[0m\n",
       "\u001b[0;34m        The minimum weighted fraction of the sum total of weights (of all\u001b[0m\n",
       "\u001b[0;34m        the input samples) required to be at a leaf node. Samples have\u001b[0m\n",
       "\u001b[0;34m        equal weight when sample_weight is not provided.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\u001b[0m\n",
       "\u001b[0;34m        The number of features to consider when looking for the best split:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - If int, then consider `max_features` features at each split.\u001b[0m\n",
       "\u001b[0;34m        - If float, then `max_features` is a fraction and\u001b[0m\n",
       "\u001b[0;34m          `max(1, int(max_features * n_features_in_))` features are considered at each\u001b[0m\n",
       "\u001b[0;34m          split.\u001b[0m\n",
       "\u001b[0;34m        - If \"auto\", then `max_features=sqrt(n_features)`.\u001b[0m\n",
       "\u001b[0;34m        - If \"sqrt\", then `max_features=sqrt(n_features)`.\u001b[0m\n",
       "\u001b[0;34m        - If \"log2\", then `max_features=log2(n_features)`.\u001b[0m\n",
       "\u001b[0;34m        - If None, then `max_features=n_features`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionchanged:: 1.1\u001b[0m\n",
       "\u001b[0;34m            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. deprecated:: 1.1\u001b[0m\n",
       "\u001b[0;34m            The `\"auto\"` option was deprecated in 1.1 and will be removed\u001b[0m\n",
       "\u001b[0;34m            in 1.3.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Note: the search for a split does not stop until at least one\u001b[0m\n",
       "\u001b[0;34m        valid partition of the node samples is found, even if it requires to\u001b[0m\n",
       "\u001b[0;34m        effectively inspect more than ``max_features`` features.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    max_leaf_nodes : int, default=None\u001b[0m\n",
       "\u001b[0;34m        Grow trees with ``max_leaf_nodes`` in best-first fashion.\u001b[0m\n",
       "\u001b[0;34m        Best nodes are defined as relative reduction in impurity.\u001b[0m\n",
       "\u001b[0;34m        If None then unlimited number of leaf nodes.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    min_impurity_decrease : float, default=0.0\u001b[0m\n",
       "\u001b[0;34m        A node will be split if this split induces a decrease of the impurity\u001b[0m\n",
       "\u001b[0;34m        greater than or equal to this value.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        The weighted impurity decrease equation is the following::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            N_t / N * (impurity - N_t_R / N_t * right_impurity\u001b[0m\n",
       "\u001b[0;34m                                - N_t_L / N_t * left_impurity)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        where ``N`` is the total number of samples, ``N_t`` is the number of\u001b[0m\n",
       "\u001b[0;34m        samples at the current node, ``N_t_L`` is the number of samples in the\u001b[0m\n",
       "\u001b[0;34m        left child, and ``N_t_R`` is the number of samples in the right child.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\u001b[0m\n",
       "\u001b[0;34m        if ``sample_weight`` is passed.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 0.19\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    bootstrap : bool, default=True\u001b[0m\n",
       "\u001b[0;34m        Whether bootstrap samples are used when building trees. If False, the\u001b[0m\n",
       "\u001b[0;34m        whole dataset is used to build each tree.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    oob_score : bool, default=False\u001b[0m\n",
       "\u001b[0;34m        Whether to use out-of-bag samples to estimate the generalization score.\u001b[0m\n",
       "\u001b[0;34m        Only available if bootstrap=True.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_jobs : int, default=None\u001b[0m\n",
       "\u001b[0;34m        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\u001b[0m\n",
       "\u001b[0;34m        :meth:`decision_path` and :meth:`apply` are all parallelized over the\u001b[0m\n",
       "\u001b[0;34m        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\u001b[0m\n",
       "\u001b[0;34m        context. ``-1`` means using all processors. See :term:`Glossary\u001b[0m\n",
       "\u001b[0;34m        <n_jobs>` for more details.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    random_state : int, RandomState instance or None, default=None\u001b[0m\n",
       "\u001b[0;34m        Controls both the randomness of the bootstrapping of the samples used\u001b[0m\n",
       "\u001b[0;34m        when building trees (if ``bootstrap=True``) and the sampling of the\u001b[0m\n",
       "\u001b[0;34m        features to consider when looking for the best split at each node\u001b[0m\n",
       "\u001b[0;34m        (if ``max_features < n_features``).\u001b[0m\n",
       "\u001b[0;34m        See :term:`Glossary <random_state>` for details.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    verbose : int, default=0\u001b[0m\n",
       "\u001b[0;34m        Controls the verbosity when fitting and predicting.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    warm_start : bool, default=False\u001b[0m\n",
       "\u001b[0;34m        When set to ``True``, reuse the solution of the previous call to fit\u001b[0m\n",
       "\u001b[0;34m        and add more estimators to the ensemble, otherwise, just fit a whole\u001b[0m\n",
       "\u001b[0;34m        new forest. See :term:`Glossary <warm_start>` and\u001b[0m\n",
       "\u001b[0;34m        :ref:`gradient_boosting_warm_start` for details.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\u001b[0m\n",
       "\u001b[0;34m            default=None\u001b[0m\n",
       "\u001b[0;34m        Weights associated with classes in the form ``{class_label: weight}``.\u001b[0m\n",
       "\u001b[0;34m        If not given, all classes are supposed to have weight one. For\u001b[0m\n",
       "\u001b[0;34m        multi-output problems, a list of dicts can be provided in the same\u001b[0m\n",
       "\u001b[0;34m        order as the columns of y.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Note that for multioutput (including multilabel) weights should be\u001b[0m\n",
       "\u001b[0;34m        defined for each class of every column in its own dict. For example,\u001b[0m\n",
       "\u001b[0;34m        for four-class multilabel classification weights should be\u001b[0m\n",
       "\u001b[0;34m        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\u001b[0m\n",
       "\u001b[0;34m        [{1:1}, {2:5}, {3:1}, {4:1}].\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        The \"balanced\" mode uses the values of y to automatically adjust\u001b[0m\n",
       "\u001b[0;34m        weights inversely proportional to class frequencies in the input data\u001b[0m\n",
       "\u001b[0;34m        as ``n_samples / (n_classes * np.bincount(y))``\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        The \"balanced_subsample\" mode is the same as \"balanced\" except that\u001b[0m\n",
       "\u001b[0;34m        weights are computed based on the bootstrap sample for every tree\u001b[0m\n",
       "\u001b[0;34m        grown.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        For multi-output, the weights of each column of y will be multiplied.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Note that these weights will be multiplied with sample_weight (passed\u001b[0m\n",
       "\u001b[0;34m        through the fit method) if sample_weight is specified.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    ccp_alpha : non-negative float, default=0.0\u001b[0m\n",
       "\u001b[0;34m        Complexity parameter used for Minimal Cost-Complexity Pruning. The\u001b[0m\n",
       "\u001b[0;34m        subtree with the largest cost complexity that is smaller than\u001b[0m\n",
       "\u001b[0;34m        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\u001b[0m\n",
       "\u001b[0;34m        :ref:`minimal_cost_complexity_pruning` for details.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 0.22\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    max_samples : int or float, default=None\u001b[0m\n",
       "\u001b[0;34m        If bootstrap is True, the number of samples to draw from X\u001b[0m\n",
       "\u001b[0;34m        to train each base estimator.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - If None (default), then draw `X.shape[0]` samples.\u001b[0m\n",
       "\u001b[0;34m        - If int, then draw `max_samples` samples.\u001b[0m\n",
       "\u001b[0;34m        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\u001b[0m\n",
       "\u001b[0;34m          `max_samples` should be in the interval `(0.0, 1.0]`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 0.22\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Attributes\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\u001b[0m\n",
       "\u001b[0;34m        The child estimator template used to create the collection of fitted\u001b[0m\n",
       "\u001b[0;34m        sub-estimators.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 1.2\u001b[0m\n",
       "\u001b[0;34m           `base_estimator_` was renamed to `estimator_`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    base_estimator_ : DecisionTreeClassifier\u001b[0m\n",
       "\u001b[0;34m        The child estimator template used to create the collection of fitted\u001b[0m\n",
       "\u001b[0;34m        sub-estimators.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. deprecated:: 1.2\u001b[0m\n",
       "\u001b[0;34m            `base_estimator_` is deprecated and will be removed in 1.4.\u001b[0m\n",
       "\u001b[0;34m            Use `estimator_` instead.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    estimators_ : list of DecisionTreeClassifier\u001b[0m\n",
       "\u001b[0;34m        The collection of fitted sub-estimators.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    classes_ : ndarray of shape (n_classes,) or a list of such arrays\u001b[0m\n",
       "\u001b[0;34m        The classes labels (single output problem), or a list of arrays of\u001b[0m\n",
       "\u001b[0;34m        class labels (multi-output problem).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_classes_ : int or list\u001b[0m\n",
       "\u001b[0;34m        The number of classes (single output problem), or a list containing the\u001b[0m\n",
       "\u001b[0;34m        number of classes for each output (multi-output problem).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_features_in_ : int\u001b[0m\n",
       "\u001b[0;34m        Number of features seen during :term:`fit`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 0.24\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[0m\n",
       "\u001b[0;34m        Names of features seen during :term:`fit`. Defined only when `X`\u001b[0m\n",
       "\u001b[0;34m        has feature names that are all strings.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 1.0\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_outputs_ : int\u001b[0m\n",
       "\u001b[0;34m        The number of outputs when ``fit`` is performed.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    feature_importances_ : ndarray of shape (n_features,)\u001b[0m\n",
       "\u001b[0;34m        The impurity-based feature importances.\u001b[0m\n",
       "\u001b[0;34m        The higher, the more important the feature.\u001b[0m\n",
       "\u001b[0;34m        The importance of a feature is computed as the (normalized)\u001b[0m\n",
       "\u001b[0;34m        total reduction of the criterion brought by that feature.  It is also\u001b[0m\n",
       "\u001b[0;34m        known as the Gini importance.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Warning: impurity-based feature importances can be misleading for\u001b[0m\n",
       "\u001b[0;34m        high cardinality features (many unique values). See\u001b[0m\n",
       "\u001b[0;34m        :func:`sklearn.inspection.permutation_importance` as an alternative.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    oob_score_ : float\u001b[0m\n",
       "\u001b[0;34m        Score of the training dataset obtained using an out-of-bag estimate.\u001b[0m\n",
       "\u001b[0;34m        This attribute exists only when ``oob_score`` is True.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \\\u001b[0m\n",
       "\u001b[0;34m            (n_samples, n_classes, n_outputs)\u001b[0m\n",
       "\u001b[0;34m        Decision function computed with out-of-bag estimate on the training\u001b[0m\n",
       "\u001b[0;34m        set. If n_estimators is small it might be possible that a data point\u001b[0m\n",
       "\u001b[0;34m        was never left out during the bootstrap. In this case,\u001b[0m\n",
       "\u001b[0;34m        `oob_decision_function_` might contain NaN. This attribute exists\u001b[0m\n",
       "\u001b[0;34m        only when ``oob_score`` is True.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    See Also\u001b[0m\n",
       "\u001b[0;34m    --------\u001b[0m\n",
       "\u001b[0;34m    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\u001b[0m\n",
       "\u001b[0;34m    sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\u001b[0m\n",
       "\u001b[0;34m        tree classifiers.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Notes\u001b[0m\n",
       "\u001b[0;34m    -----\u001b[0m\n",
       "\u001b[0;34m    The default values for the parameters controlling the size of the trees\u001b[0m\n",
       "\u001b[0;34m    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\u001b[0m\n",
       "\u001b[0;34m    unpruned trees which can potentially be very large on some data sets. To\u001b[0m\n",
       "\u001b[0;34m    reduce memory consumption, the complexity and size of the trees should be\u001b[0m\n",
       "\u001b[0;34m    controlled by setting those parameter values.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    The features are always randomly permuted at each split. Therefore,\u001b[0m\n",
       "\u001b[0;34m    the best found split may vary, even with the same training data,\u001b[0m\n",
       "\u001b[0;34m    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\u001b[0m\n",
       "\u001b[0;34m    of the criterion is identical for several splits enumerated during the\u001b[0m\n",
       "\u001b[0;34m    search of the best split. To obtain a deterministic behaviour during\u001b[0m\n",
       "\u001b[0;34m    fitting, ``random_state`` has to be fixed.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    References\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Examples\u001b[0m\n",
       "\u001b[0;34m    --------\u001b[0m\n",
       "\u001b[0;34m    >>> from sklearn.ensemble import RandomForestClassifier\u001b[0m\n",
       "\u001b[0;34m    >>> from sklearn.datasets import make_classification\u001b[0m\n",
       "\u001b[0;34m    >>> X, y = make_classification(n_samples=1000, n_features=4,\u001b[0m\n",
       "\u001b[0;34m    ...                            n_informative=2, n_redundant=0,\u001b[0m\n",
       "\u001b[0;34m    ...                            random_state=0, shuffle=False)\u001b[0m\n",
       "\u001b[0;34m    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\u001b[0m\n",
       "\u001b[0;34m    >>> clf.fit(X, y)\u001b[0m\n",
       "\u001b[0;34m    RandomForestClassifier(...)\u001b[0m\n",
       "\u001b[0;34m    >>> print(clf.predict([[0, 0, 0, 0]]))\u001b[0m\n",
       "\u001b[0;34m    [1]\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mForestClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"class_weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mStrOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"balanced_subsample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"splitter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sqrt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mestimator_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"criterion\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_depth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_samples_split\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_samples_leaf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_weight_fraction_leaf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_features\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_leaf_nodes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_impurity_decrease\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"random_state\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"ccp_alpha\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_impurity_decrease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mccp_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccp_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda/envs/nobleprog_training/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=10,\n",
    "                               max_samples = 0.7,\n",
    "                               max_features= 0.8,\n",
    "                               max_depth=3,\n",
    "                               min_samples_split=2,\n",
    "                               random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=3, max_features=0.8, max_samples=0.7,\n",
       "                       n_estimators=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=3, max_features=0.8, max_samples=0.7,\n",
       "                       n_estimators=10, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=3, max_features=0.8, max_samples=0.7,\n",
       "                       n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8103448275862069 0.6911764705882353 0.746031746031746\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_valid)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_valid, y_pred, average='binary')\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz: can you try using grid search to find the best n_estimator, max_samples, and max_features?\n",
    "\n",
    "Hint: refer to the previous nb to find how to do grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 3, 'max_features': 0.4, 'max_samples': 0.7, 'min_samples_split': 2, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_estimators': [5, 10, 15, 20], 'max_features': [0.2, 0.4, 0.6, 0.8],\n",
    "              'max_samples': [0.5, 0.7, 0.9], 'max_depth': [3], 'min_samples_split':[2]}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(**grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095238095238095 0.75 0.7786259541984734\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_valid)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_valid, y_pred, average='binary')\n",
    "print(precision, recall, f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nobleprog_training",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
